{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing the requirements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:50:47.329127Z",
     "end_time": "2023-06-06T19:50:49.862468Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from keybert import KeyBERT\n",
    "from multi_rake import Rake\n",
    "import yake\n",
    "import os\n",
    "import collections\n",
    "import json\n",
    "# import num2words\n",
    "import re\n",
    "import datetime\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:50:49.866474Z",
     "end_time": "2023-06-06T19:50:49.874473Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setups"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keyword Extractors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dt = datetime.datetime.now()\n",
    "timestamp = dt.strftime('%Y_%m_%d_%H:%M:%S')[:-3]\n",
    "\n",
    "kw_model = KeyBERT(model='all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "def kw_yake(text):\n",
    "    # kw_extractor = yake.KeywordExtractor()\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 1\n",
    "    deduplication_threshold = 0.9\n",
    "    numOfKeywords = 10\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold,\n",
    "                                                top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    keywords_str = \"\"\n",
    "    for index, keyword in enumerate(keywords):\n",
    "        if index == len(keywords) - 1:\n",
    "            keywords_str += f\"{keyword[0]}\"\n",
    "        else:\n",
    "            keywords_str += f\"{keyword[0]} \"\n",
    "    return keywords_str\n",
    "\n",
    "\n",
    "def kw_yake_detailed(text):\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 5\n",
    "    deduplication_threshold = 0.0\n",
    "    numOfKeywords = 10\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold,\n",
    "                                                top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    keywords_str = \"\"\n",
    "    for index, keyword in enumerate(keywords):\n",
    "        if index == len(keywords) - 1:\n",
    "            keywords_str += f\"{keyword[0]}\"\n",
    "        else:\n",
    "            keywords_str += f\"{keyword[0]} \"\n",
    "    return keywords_str\n",
    "\n",
    "\n",
    "def kw_rake(text):\n",
    "    rake = Rake()\n",
    "    keywords = rake.apply(text)\n",
    "    keywords_str = \"\"\n",
    "    for index, keyword in enumerate(keywords):\n",
    "        if index == len(keywords) - 1:\n",
    "            keywords_str += f\"{keyword[0]}\"\n",
    "        else:\n",
    "            keywords_str += f\"{keyword[0]} \"\n",
    "    return (keywords_str)\n",
    "\n",
    "\n",
    "def kw_keybert(text):\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 7),\n",
    "        stop_words='english',\n",
    "        highlight=False,\n",
    "        top_n=1)\n",
    "    keywords_list = list(dict(keywords).keys())\n",
    "    if not keywords_list:\n",
    "        return \"\"\n",
    "    return (keywords_list[0])\n",
    "\n",
    "\n",
    "def keyword_extractors(text):\n",
    "    keywords = ({\n",
    "        \"KeyBERT\": kw_keybert(text),\n",
    "        \"RAKE\": kw_rake(text),\n",
    "        \"YAKE\": kw_yake(text),\n",
    "        \"YAKE (Detailed)\": kw_yake_detailed(text),\n",
    "    })\n",
    "    return keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:50:49.872641Z",
     "end_time": "2023-06-06T19:50:51.045061Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decontraction Function and slicer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"Won\\'t\", \"Will not\", phrase)\n",
    "    phrase = re.sub(r\"Can\\'t\", \"Can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "list_negations = [\n",
    "    \" not \",\n",
    "    \" not.\",\n",
    "    \" no \",\n",
    "    \" no.\"\n",
    "]\n",
    "\n",
    "def slicer(my_str, sub):\n",
    "    index = my_str.find(sub)\n",
    "    if index != -1:\n",
    "        return my_str[index + len(sub):]\n",
    "    else:\n",
    "        raise Exception('Sub string not found!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:50:51.050257Z",
     "end_time": "2023-06-06T19:50:51.052915Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Folder Paths and reading the raw files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "folderpath = r\"./txt_conversations\"  # make sure to put the 'r' in front\n",
    "folderpath_stripped = folderpath.replace(\"./\",\"\")\n",
    "\n",
    "filepaths = [os.path.join(folderpath, name) for name in os.listdir(folderpath)]\n",
    "all_files = []\n",
    "\n",
    "for path in filepaths:\n",
    "    print(path)\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            file = f.readlines()\n",
    "            all_files.append(file)\n",
    "    except:\n",
    "        print(\"EXCEPTION\" + path)\n",
    "        continue\n",
    "\n",
    "files_as_text = []\n",
    "for elements in all_files:\n",
    "    text = \"\"\n",
    "    for lines in elements:\n",
    "        try:\n",
    "            text += lines\n",
    "        except:\n",
    "            print(\"Exception in all files\")\n",
    "            continue\n",
    "    files_as_text.append(text)\n",
    "\n",
    "with open('./output_files/all_text.json', mode='w', encoding='utf-8') as feedsjson2:\n",
    "    json.dump(all_files, feedsjson2)\n",
    "\n",
    "with open('./output_files/all_text_array.json', mode='w', encoding='utf-8') as feedsjson3:\n",
    "    json.dump(files_as_text, feedsjson3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:50:51.055202Z",
     "end_time": "2023-06-06T19:50:51.062056Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Declaring variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feeds = collections.defaultdict(list)\n",
    "feeds2 = []\n",
    "current = \"\"\n",
    "files_total = 0\n",
    "lines_total = 0\n",
    "skipped = 0\n",
    "not_skipped_answers = 0\n",
    "skipped_too_long = 0\n",
    "skipped_too_long_cache = 0\n",
    "not_skipped_questions = 0\n",
    "empty_line = 0\n",
    "keywords_counter = ({\n",
    "        \"KeyBERT\": 0,\n",
    "        \"RAKE\": 0,\n",
    "        \"YAKE\": 0,\n",
    "        'YAKE_DETAILED': 0\n",
    "    })\n",
    "\n",
    "history_counter = ({\n",
    "        \"0\": 0,\n",
    "        \"1\": 0,\n",
    "        \"2\": 0,\n",
    "        '3': 0\n",
    "    })\n",
    "\n",
    "length = len(all_files)\n",
    "counter_1 = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:50:51.063249Z",
     "end_time": "2023-06-06T19:50:51.088634Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Iterating through all files and create the dataset structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for fp in all_files:\n",
    "    counter_1 += 1\n",
    "    print(f\"______________## {(counter_1/length)*100} % ##______________\")\n",
    "    http_in_file = False\n",
    "    for lines in fp:\n",
    "        if \"http\" in lines:\n",
    "            print(\"+++ HTTP IN LINE +++\")\n",
    "            http_in_file = True\n",
    "    files_total += 1\n",
    "    current = \"\"\n",
    "    lines = []\n",
    "    counter = 0\n",
    "    cache = \"\"\n",
    "    cache_happy_transformers = \"\"\n",
    "    cache_array = []\n",
    "    http_here = False\n",
    "\n",
    "    for i, line in enumerate(fp):\n",
    "        ## ALTERNATIVE 1 ##\n",
    "        # For all conversations where the history is longer than 900 chars, the conversation is skipped\n",
    "        # if len(cache) > 900:\n",
    "        #     skipped_too_long_cache +=1\n",
    "        #     continue\n",
    "        ## ALTERNATIVE 1 ##\n",
    "\n",
    "\n",
    "        ## ALTERNATIVE 2 ##\n",
    "        # It is randomly chosen, how long the appended history in a conversation is\n",
    "        # It is never longer than 3 QA-Pairs\n",
    "        if len(cache_array) == 1:\n",
    "            pop_random = random.randint(1, 5)\n",
    "            if pop_random == 1:\n",
    "                cache_array.clear()\n",
    "\n",
    "        if len(cache_array) == 2:\n",
    "            pop_random = random.randint(0, 5)\n",
    "            if pop_random == 1:\n",
    "                cache_array.pop(0)\n",
    "            elif pop_random == 2:\n",
    "                cache_array.pop(0)\n",
    "                cache_array.pop(0)\n",
    "            elif pop_random == 3:\n",
    "                cache_array.clear()\n",
    "\n",
    "        if len(cache_array) > 3:\n",
    "            pop_random = random.randint(1, 3)\n",
    "            if pop_random == 1:\n",
    "                cache_array.pop(0)\n",
    "            elif pop_random == 2:\n",
    "                cache_array.pop(0)\n",
    "                cache_array.pop(0)\n",
    "            elif pop_random == 3:\n",
    "                cache_array.clear()\n",
    "        ## ALTERNATIVE 2 ##\n",
    "\n",
    "        lines_total += 1\n",
    "\n",
    "        # Sometimes there are HTTP links or empty lines in the conversations -> Those are skipped\n",
    "        if http_in_file:\n",
    "            if line == \"\" or line == \"\\n\" or line == \"\\r\" or http_here is False:\n",
    "                if \"http\" in line:\n",
    "                    http_here = True\n",
    "                print(f\"+++ Empty Line found in HTTP  {fp} +++\")\n",
    "                empty_line += 1\n",
    "                continue\n",
    "        elif line == \"\" or line == \"\\n\" or line == \"\\r\" or \": \" not in line:\n",
    "            print(f\"+++ Empty Line found in {fp} +++\")\n",
    "            empty_line += 1\n",
    "            continue\n",
    "        else:\n",
    "            line = slicer(line.strip(), \": \")\n",
    "\n",
    "        # If the random number == 3 -> The conversation is skipped\n",
    "        # Setting the interval between 0 and 2 leads to three different trainingfiles equal in size\n",
    "        random_number = random.randint(0, 2)\n",
    "        if random_number != 3:\n",
    "            skip = random_number\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "        line = line.replace(\"\\\"\", \"\").replace(\"’\", \"'\").replace(\"–\",\"-\").replace(\"“\", \"\\\"\").replace(\" \", \" \").replace(\"“\", \"\\n\").replace(\"é\",\"é\").replace(\"  \", \" \").replace(\"…\", \"...\").replace(\"‘\", \"'\").replace(\"é\",\"é\")\n",
    "\n",
    "        # Handling Questions and Answers in the Conversation file\n",
    "        if (counter % 2) == 0 and not current == \"question\":\n",
    "            # Saving the question to append it later on\n",
    "            not_skipped_questions += 1\n",
    "            question = line\n",
    "            current = \"question\"\n",
    "        elif current == \"question\":\n",
    "            ## If a \"?\" is at the first answer, every second conversation will be skipped ##\n",
    "            if counter == 1 and \"?\" in line:\n",
    "                print(\"??? Fragezeichen erste Antwort ???\")\n",
    "                if random.randint(0,1):\n",
    "                    print(\"??? SKIPPED ???\")\n",
    "                    break\n",
    "            ## If a \"?\" is at the first answer, every second conversation will be skipped ##\n",
    "\n",
    "            current = \"answer\"\n",
    "            answer = line\n",
    "\n",
    "            # If the answer is too long, it will be skipped\n",
    "            if len(answer) > 110:\n",
    "                skipped_too_long += 1\n",
    "                skipped += 1\n",
    "                skip = 3\n",
    "\n",
    "            # GPT3 Format ##\n",
    "            if not skip == 3:\n",
    "                not_skipped_answers += 1\n",
    "                answer_prepared = decontracted(answer)\n",
    "                random_keyword_generator = random.randint(1, 4)\n",
    "                keywords = \"\"\n",
    "\n",
    "\n",
    "                ## Alternative RND: Random Keyword Model Chosen ##\n",
    "                if random_keyword_generator == 1:\n",
    "                    keywords = kw_keybert(answer_prepared)\n",
    "                    keywords_counter['KeyBERT'] += 1\n",
    "                elif random_keyword_generator == 2:\n",
    "                    keywords = kw_rake(answer_prepared)\n",
    "                    keywords_counter['RAKE'] += 1\n",
    "                elif random_keyword_generator == 3:\n",
    "                    keywords = kw_yake(answer_prepared)\n",
    "                    keywords_counter['YAKE'] += 1\n",
    "                else:\n",
    "                    keywords = kw_yake_detailed(answer_prepared)\n",
    "                    keywords_counter['YAKE_DETAILED'] += 1\n",
    "                ## Alternative RND: Random Keyword Model Chosen ##\n",
    "\n",
    "                ## Alternative CR: Choose the shortest keywords ##\n",
    "                # for x in range(4):\n",
    "                #     if x == 0:\n",
    "                #         keywords_attempt = kw_keybert(answer_prepared)\n",
    "                #         if len(keywords_attempt) < len(keywords) or len(keywords) == 0:\n",
    "                #             print(f\"+++ KW SHORTENED +++ (WAS: {keywords} IS NOW: {keywords_attempt}\")\n",
    "                #             keywords = keywords_attempt\n",
    "                #     elif x == 1:\n",
    "                #         keywords_attempt = kw_rake(answer_prepared)\n",
    "                #         if len(keywords_attempt) < len(keywords) or len(keywords) == 0:\n",
    "                #             print(f\"+++ KW SHORTENED +++ (WAS: {keywords} IS NOW: {keywords_attempt}\")\n",
    "                #             keywords = keywords_attempt\n",
    "                #     elif x == 2:\n",
    "                #         keywords_attempt = kw_yake(answer_prepared)\n",
    "                #         if len(keywords_attempt) < len(keywords) or len(keywords) == 0:\n",
    "                #             print(f\"+++ KW SHORTENED +++ (WAS: {keywords} IS NOW: {keywords_attempt}\")\n",
    "                #             keywords = keywords_attempt\n",
    "                #     else:\n",
    "                #         keywords_attempt = kw_yake_detailed(answer_prepared)\n",
    "                #         if len(keywords_attempt) < len(keywords) or len(keywords) == 0:\n",
    "                #             print(f\"+++ KW SHORTENED +++ (WAS: {keywords} IS NOW: {keywords_attempt}\")\n",
    "                #             keywords = keywords_attempt\n",
    "                ## Alternative CR: Choose the shortest keywords ##\n",
    "\n",
    "                # If the random chosen keyword extractor of alternative 1 did not find a keyword, the other extractors try to find one\n",
    "                if keywords == \"\":\n",
    "                    print(\"+++ Keyword Extraction -> Empty Result +++\")\n",
    "                    for x in range(4):\n",
    "                        if keywords:\n",
    "                            print(f\"+++ KW FOUND +++ (By another extraxtor)\")\n",
    "                            break\n",
    "                        if x == 0:\n",
    "                            keywords = kw_keybert(answer_prepared)\n",
    "                        elif x == 1:\n",
    "                            keywords = kw_rake(answer_prepared)\n",
    "                        elif x == 2:\n",
    "                            keywords = kw_yake(answer_prepared)\n",
    "                        else:\n",
    "                            keywords = kw_yake_detailed(answer_prepared)\n",
    "                if keywords == \"\":\n",
    "                    print(f\"+++ KW still not found +++ (No KWs found at all)\")\n",
    "                    skip = 3\n",
    "\n",
    "\n",
    "                if not skip == 3:\n",
    "                ## Check if the answer contains a negation\n",
    "                    for negated_word in list_negations:\n",
    "                        if negated_word in answer_prepared.lower():\n",
    "                            # Check if the negation is present in the keywords\n",
    "                            if negated_word not in keywords.lower():\n",
    "                                ## Check if the negation is rather in the beginning or the end and insert it in the keywords\n",
    "                                if answer_prepared.find(negated_word) < (len(answer_prepared) - len(negated_word))/2:\n",
    "                                    keywords = negated_word.replace(\" \", \"\").replace(\".\", \"\") + \" \" + keywords\n",
    "                                else:\n",
    "                                    keywords = keywords + \" \" +  negated_word.replace(\" \", \"\").replace(\".\", \"\")\n",
    "\n",
    "                    keywords_all = keyword_extractors(answer_prepared)\n",
    "\n",
    "                    filename = f\"./output_files/{timestamp}_part_{skip}_{folderpath_stripped}_training.json\"\n",
    "                    with open(filename, mode='w', encoding='utf-8') as feedsjson:\n",
    "                        ## EVERY SNIPPET AS QA PAIR ONLY WITHOUT HISTORY ##\n",
    "                        # entry = {'prompt': f\"Question: {question}\\nKeywords: {keywords}\\nAnswer:\\n\\n###\\n\\n\", 'completion': f\"{answer} END\" }\n",
    "                        # feeds.append(entry)\n",
    "                        # json.dump(feeds, feedsjson)\n",
    "                        ## EVERY SNIPPET AS QA PAIR ONLY WITHOUT HISTORY ##\n",
    "\n",
    "                        ## ALTERNATIVE WITH CACHE ARRAY ##\n",
    "                        history = \"\"\n",
    "                        for entries in cache_array:\n",
    "                            history += entries\n",
    "                        history_counter[str(len(cache_array))] += 1\n",
    "                        entry2 = {'prompt': f\"{history}Question: {question}\\nKeywords: {keywords}\\nAnswer:\\n\\n###\\n\\n\",\n",
    "                                  'completion': f\"{answer} END\"}\n",
    "                        ## ALTERNATIVE WITH CACHE ARRAY ##\n",
    "\n",
    "                        ## ALTERNATIVE W/O CACHE ARRAY ##\n",
    "                        # entry2 = {'prompt': f\"{cache}Question: {question}\\nKeywords: {keywords}\\nAnswer:\\n\\n###\\n\\n\",\n",
    "                        #           'completion': f\"{answer} END\"}\n",
    "                        ## ALTERNATIVE W/O CACHE ARRAY ##\n",
    "\n",
    "                        # Appending different arrays to have separate training files\n",
    "                        feeds['all'].append(entry2)\n",
    "                        feeds[skip].append(entry2)\n",
    "                        json.dump(feeds[skip], feedsjson)\n",
    "\n",
    "                    filename = f\"./output_files/{timestamp}_all_{folderpath_stripped}_training.json\"\n",
    "                    with open(filename, mode='w', encoding='utf-8') as feedsjson3:\n",
    "                        json.dump(feeds['all'], feedsjson3)\n",
    "\n",
    "                    ## Save keywords to compare the methods later on\n",
    "                    with open('./output_files/keyword_examples.json', mode='w', encoding='utf-8') as feedsjson2:\n",
    "                        entry2 = {'sentence': f\"{answer}\", 'keywords': keywords_all}\n",
    "                        feeds2.append(entry2)\n",
    "\n",
    "                        # For the whole conversation ##\n",
    "                        json.dump(feeds2, feedsjson2)\n",
    "            cache += f\"Question: {question}\\nAnswer: {answer}\\n\"\n",
    "            cache_array.append(f\"Question: {question}\\nAnswer: {answer}\\n\")\n",
    "        else:\n",
    "            print(\"+++ Skipped the whole file because order was mixed +++\")\n",
    "            print(\"Current: \"+ current +\" line that made it skip: Number: \" + str(counter) + line)\n",
    "            break\n",
    "        counter += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:50:51.066338Z",
     "end_time": "2023-06-06T19:51:05.193449Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Printing some stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Files in total: \" + str(files_total))\n",
    "print(\"Lines in total : \" + str(lines_total))\n",
    "print(\"Empty Lines in total : \" + str(empty_line))\n",
    "print(\"Skipped too long cache : \" + str(skipped_too_long_cache))\n",
    "print(\"Skipped: \" + str(skipped))\n",
    "print(\"Skipped Too long: \" + str(skipped_too_long))\n",
    "print(\"Not Skipped Answers : \" + str(not_skipped_answers))\n",
    "print(\"Not Skipped Questions: \" + str(not_skipped_questions))\n",
    "print(\"Keywords counter: \" + str(keywords_counter))\n",
    "print(\"History counter: \" + str(history_counter))\n",
    "for all_elements in (feeds):\n",
    "    print(f\"Length of training split {all_elements}: {len(feeds[all_elements])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-06T19:51:05.196168Z",
     "end_time": "2023-06-06T19:51:05.198948Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
